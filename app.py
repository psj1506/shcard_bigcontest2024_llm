import os
import numpy as np
import pandas as pd
from transformers import AutoTokenizer, AutoModel
import torch
import faiss
import streamlit as st
import google.generativeai as genai

# ê²½ë¡œ ì„¤ì •
data_path = './data'
module_path = './modules'

# Gemini ëª¨ë¸ ì„¤ì •
genai.configure(api_key="YOUR_API_KEY")
model = genai.GenerativeModel("gemini-1.5-flash")

# ë°ì´í„° ë¡œë“œ
df = pd.read_csv(os.path.join(data_path, "JEJU_DATA.csv"), encoding='cp949')
df_tour = pd.read_csv(os.path.join(data_path, "JEJU_TOUR.csv"), encoding='cp949')
text_tour = df_tour['text'].tolist()

# ìµœì‹ ì—°ì›” ë°ì´í„°ë§Œ ì‚¬ìš© (í•„í„°ë§ëœ ë°ì´í„°í”„ë ˆì„ìœ¼ë¡œ ì¼ê´€ë˜ê²Œ ì‚¬ìš©)
df = df.loc[df.groupby('ê°€ë§¹ì ëª…')['ê¸°ì¤€ì—°ì›”'].idxmax()].reset_index(drop=True)

# Streamlit App UI ì„¤ì •
st.set_page_config(page_title="ğŸŠì œì£¼ë„ ë§›ì§‘ ì¶”ì²œ")

# ì‚¬ì´ë“œë°” ì„¤ì •
with st.sidebar:
    st.title("**ğŸŠì œì£¼ë„ ë§›ì§‘ ì¶”ì²œ**")
    st.sidebar.markdown('<p class="sidebar-text">ğŸ’µí¬ë§ ê°€ê²©ëŒ€ëŠ” ì–´ë–»ê²Œ ë˜ì‹œë‚˜ìš”??</p>', unsafe_allow_html=True)
    
    price_options = ['ğŸ‘Œ ìƒê´€ ì—†ìŒ', 'ğŸ˜ ìµœê³ ê°€', 'ğŸ’¸ ê³ ê°€', 'ğŸ’° í‰ê·  ê°€ê²©ëŒ€', 'ğŸ’µ ì¤‘ì €ê°€', 'ğŸ˜‚ ì €ê°€']
    price_mapping = {
        'ğŸ‘Œ ìƒê´€ ì—†ìŒ': 'ìƒê´€ ì—†ìŒ',
        'ğŸ˜ ìµœê³ ê°€': '6',
        'ğŸ’¸ ê³ ê°€': '5',
        'ğŸ’° í‰ê·  ê°€ê²©ëŒ€': ('3', '4'),
        'ğŸ’µ ì¤‘ì €ê°€': '2',
        'ğŸ˜‚ ì €ê°€': '1'
    }
    selected_price = st.sidebar.selectbox("", price_options, key="price")
    price = price_mapping.get(selected_price, 'ìƒê´€ ì—†ìŒ')

st.title("ì–´ì„œ ì™€ìš©!ğŸ‘‹")
st.subheader("ì¸ê¸° ìˆëŠ” :orange[ì œì£¼ ë§›ì§‘]ğŸ½ï¸ğŸ˜ í›„íšŒëŠ” ì—†ì„ê±¸?!")

# ëŒ€í™” ìƒíƒœ ì´ˆê¸°í™”
if "messages" not in st.session_state:
    st.session_state.messages = [{"role": "assistant", "content": "ì–´ë–¤ ì‹ë‹¹ ì°¾ìœ¼ì‹œë‚˜ìš”?? ìœ„ì¹˜, ì—…ì¢… ë“±ì„ ì•Œë ¤ì£¼ì‹œë©´ ìµœê³ ì˜ ë§›ì§‘ ì¶”ì²œí•´ë“œë¦´ê²Œìš”!"}]

# ë””ë°”ì´ìŠ¤ ì„¤ì •
device = "cuda" if torch.cuda.is_available() else "cpu"

# Hugging Face ì„ë² ë”© ëª¨ë¸ ë° í† í¬ë‚˜ì´ì € ë¡œë“œ
model_name = "jhgan/ko-sroberta-multitask"
tokenizer = AutoTokenizer.from_pretrained(model_name)
embedding_model = AutoModel.from_pretrained(model_name).to(device)

# FAISS ì¸ë±ìŠ¤ ë¡œë“œ í•¨ìˆ˜
def load_faiss_index(index_path=os.path.join(module_path, 'faiss_index.index')):
    if os.path.exists(index_path):
        return faiss.read_index(index_path)
    else:
        raise FileNotFoundError(f"ì¸ë±ìŠ¤ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {index_path}")

# í…ìŠ¤íŠ¸ ì„ë² ë”© ìƒì„±
def embed_text(text):
    inputs = tokenizer(text, return_tensors='pt', padding=True, truncation=True).to(device)
    with torch.no_grad():
        embeddings = embedding_model(**inputs).last_hidden_state.mean(dim=1)
    return embeddings.squeeze().cpu().numpy()

# FAISSë¥¼ í™œìš©í•œ ì‘ë‹µ ìƒì„±
def generate_response_with_faiss(question, df, embeddings, model, df_tour, embeddings_tour, max_count=10, k=3, print_prompt=True):
    index = load_faiss_index()
    index.nprobe = 10
    query_embedding = embed_text(question).reshape(1, -1)
    distances, indices = index.search(query_embedding, 10)

    index_tour = load_faiss_index(index_path=os.path.join(module_path, 'faiss_tour_index.index'))
    query_embedding_tour = embed_text(question).reshape(1, -1)
    distances_tour, indices_tour = index_tour.search(query_embedding_tour, 1)

    filtered_df = df.iloc[indices[0, :]].reset_index(drop=True)
    filtered_df_tour = df_tour.iloc[indices_tour[0, :]].reset_index(drop=True)

    # ê°€ê²©ëŒ€ í•„í„°ë§ (íŠœí”Œì„ ì‚¬ìš©í•´ íŠ¹ì • ê°€ê²©ëŒ€ êµ¬ê°„ í¬í•¨)
    if price != 'ìƒê´€ ì—†ìŒ':
        if isinstance(price, tuple):
            filtered_df = filtered_df[filtered_df['ê±´ë‹¹í‰ê· ì´ìš©ê¸ˆì•¡êµ¬ê°„'].str.startswith(price)].reset_index(drop=True)
        else:
            filtered_df = filtered_df[filtered_df['ê±´ë‹¹í‰ê· ì´ìš©ê¸ˆì•¡êµ¬ê°„'].str.startswith(price)].reset_index(drop=True)

    if filtered_df.empty:
        return "ì§ˆë¬¸ê³¼ ì¼ì¹˜í•˜ëŠ” ê°€ê²Œê°€ ì—†ìŠµë‹ˆë‹¤."

    reference_info = "\n".join(filtered_df['text'])
    reference_tour = "\n".join(filtered_df_tour['text'])

    prompt = f"""ì§ˆë¬¸: {question}
ëŒ€ë‹µì— ì°¸ê³ í•  ì§€ì¹¨:
1. ê°€ë§¹ì ëª…ê³¼ ê°€ë§¹ì ì—…ì¢… ì •ë³´ë¥¼ ì‚¬ìš©í•˜ì—¬ ì§ˆë¬¸ì— ì–¸ê¸‰ëœ ì—…ì¢…ì´ë‚˜ íŠ¹ì • ìš”ë¦¬ë¥¼ ì œê³µí•˜ëŠ” ì‹ë‹¹ë§Œ ì¶”ì²œí•´ì¤˜.
2. ì‚¬ìš©ìê°€ ì›í•˜ëŠ” ìœ„ì¹˜(ì£¼ì†Œ í¬í•¨)ë¥¼ ê¸°ì¤€ìœ¼ë¡œ í•´ë‹¹ ì§€ì—­ ë‚´ì˜ ì‹ë‹¹ë§Œ ì¶”ì²œí•´ì¤˜.
3. ì°¨ë¡œ ì´ë™ì‹œê°„ì´ ì–¼ë§ˆì¸ì§€ ì•Œë ¤ì¤˜. ì¶”ì²œí•´ì¤„ë•Œ ì´ë™ì‹œê°„ì„ ê³ ë ¤í•´ì„œ ë‹µë³€í•´ì¤˜.
4. ì§ˆë¬¸ì— ì–¸ê¸‰ëœ ì—…ì¢…ì´ë‚˜ íŠ¹ì • ìš”ë¦¬ê°€ ì—†ëŠ” ê²½ìš° ì—…ì¢…ë³„ë¡œ í•˜ë‚˜ì”© ì¶”ì²œí•´ì¤˜.
5. ì´ìš©ê±´ìˆ˜ ë° ì—°ë ¹ëŒ€, ì„±ë³„ ë¹„ì¤‘ì„ ì‚¬ìš©í•˜ì—¬ íŠ¹ì • ì´ìš©ì¸µ(ì˜ˆ: 20ëŒ€, ì—¬ì„±)ì´ ë§ì€ ê°€ê²Œë¡œ ì¶”ì²œí•©ë‹ˆë‹¤. ëŒ€ë‹µí•´ì¤„ë•Œ ì—…ì¢…ë³„ë¡œ ê°€ëŠ¥í•˜ë©´ í•˜ë‚˜ì”© ì¶”ì²œí•´ì¤˜.
6. ê·¸ ì™¸ì—ë„ ê°€ë§¹ì ê°œì ì¼ìê°€ ì˜¤ë˜ë˜ê³  ì´ìš©ê±´ìˆ˜ê°€ ë§ì€ ìŒì‹ì (ì˜¤ë˜ëœë§›ì§‘)ê³¼ ê°€ë§¹ì ê°œì ì¼ìê°€ ìµœê·¼ì´ê³  ì´ìš©ê±´ìˆ˜ê°€ ë§ì€ ìŒì‹ì (ìƒˆë¡œìš´ë§›ì§‘)ì„ ê°ê° ì¶”ì²œí•´ì¤¬ìœ¼ë©´ ì¢‹ê² ì–´.
ì°¸ê³ í•  ìŒì‹ì  ì •ë³´: {reference_info}
ì°¸ê³ í•  ê´€ê´‘ì§€ ì •ë³´: {reference_tour}
ì‘ë‹µ:"""

    if print_prompt:
        print('-----------------------------' * 3)
        print(prompt)
        print('-----------------------------' * 3)

    response = model.generate_content(prompt)
    return response.text if hasattr(response, 'text') else response

# ì‚¬ìš©ì ì…ë ¥ ì²˜ë¦¬ ë° ì‘ë‹µ ìƒì„±
if prompt := st.chat_input():
    st.session_state.messages.append({"role": "user", "content": prompt})
    with st.chat_message("user"):
        st.write(prompt)

    with st.chat_message("assistant"):
        with st.spinner("ìƒê° ì¤‘..."):
            response = generate_response_with_faiss(prompt, df, embeddings, model, df_tour, embeddings_tour)
            st.write(response)
    st.session_state.messages.append({"role": "assistant", "content": response})
