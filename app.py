import os
import numpy as np
import pandas as pd
from transformers import AutoTokenizer, AutoModel
import torch
import faiss
import streamlit as st
import google.generativeai as genai
import logging
import re
import pynvml  # GPU ë©”ëª¨ë¦¬ í™•ì¸ì„ ìœ„í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬ ì¶”ê°€

# ê²½ë¡œ ì„¤ì •
data_path = './data'
module_path = './modules'

# ë¡œê·¸ ì„¤ì •
logging.basicConfig(filename='chatbot_logs.log', level=logging.INFO, 
                    format='%(asctime)s:%(levelname)s:%(message)s')

# GPU ë©”ëª¨ë¦¬ í™•ì¸ í•¨ìˆ˜
def get_available_gpu_memory():
    try:
        pynvml.nvmlInit()
        handle = pynvml.nvmlDeviceGetHandleByIndex(0)  # ì²« ë²ˆì§¸ GPU
        info = pynvml.nvmlDeviceGetMemoryInfo(handle)
        available_memory = info.free / (1024 ** 3)  # GB ë‹¨ìœ„
        pynvml.nvmlShutdown()
        return available_memory
    except Exception as e:
        logging.error(f"GPU ë©”ëª¨ë¦¬ í™•ì¸ ì‹¤íŒ¨: {e}")
        return 0

# ë””ë°”ì´ìŠ¤ ì„¤ì • ìµœì í™”
available_memory = get_available_gpu_memory()
required_memory = 2  # ì˜ˆ: ëª¨ë¸ ì‹¤í–‰ì— í•„ìš”í•œ ìµœì†Œ ë©”ëª¨ë¦¬ (GB ë‹¨ìœ„)

if torch.cuda.is_available() and available_memory > required_memory:
    device = "cuda"
    logging.info("GPU ì‚¬ìš© ì„¤ì •ë¨.")
else:
    device = "cpu"
    logging.info("CPU ì‚¬ìš© ì„¤ì •ë¨.")

# Gemini ëª¨ë¸ ì„¤ì • (ë³´ì•ˆ ê°•í™”)
GOOGLE_API_KEY = st.secrets["API_KEY"]
genai.configure(api_key=GOOGLE_API_KEY)
model = genai.GenerativeModel("gemini-1.5-flash")

# ë°ì´í„° ë¡œë“œ ë° í•„í„°ë§
df = pd.read_csv(os.path.join(data_path, "JEJU_DATA.csv"), encoding='cp949')
df_tour = pd.read_csv(os.path.join(data_path, "JEJU_TOUR.csv"), encoding='cp949')
text_tour = df_tour['text'].tolist()

# ìµœì‹ ì—°ì›” ë°ì´í„°ë§Œ ì‚¬ìš©
df = df.loc[df.groupby('ê°€ë§¹ì ëª…')['ê¸°ì¤€ì—°ì›”'].idxmax()].reset_index(drop=True)

# 'text' ì»¬ëŸ¼ ì¡´ì¬ í™•ì¸
if 'text' not in df.columns:
    st.error("ë°ì´í„°ì…‹ì— 'text' ì»¬ëŸ¼ì´ ì—†ìŠµë‹ˆë‹¤.")
    st.stop()

# ì„ë² ë”© ëª¨ë¸ ë° í† í¬ë‚˜ì´ì € ë¡œë“œ
model_name = "jhgan/ko-sroberta-multitask"
try:
    tokenizer = AutoTokenizer.from_pretrained(model_name)
    embedding_model = AutoModel.from_pretrained(model_name).to(device)
    logging.info("ì„ë² ë”© ëª¨ë¸ ë° í† í¬ë‚˜ì´ì € ë¡œë“œ ì™„ë£Œ.")
except Exception as e:
    logging.error(f"ì„ë² ë”© ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨: {e}")
    st.error("ì„ë² ë”© ëª¨ë¸ ë¡œë“œ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤.")
    st.stop()

# FAISS ì¸ë±ìŠ¤ ë¡œë“œ í•¨ìˆ˜ ì •ì˜
def load_faiss_index(index_path=os.path.join(module_path, 'faiss_index_1.index')):
    if os.path.exists(index_path):
        try:
            index = faiss.read_index(index_path)
            logging.info(f"FAISS ì¸ë±ìŠ¤ ë¡œë“œ ì„±ê³µ: {index_path}")
            return index
        except Exception as e:
            logging.error(f"FAISS ì¸ë±ìŠ¤ ë¡œë“œ ì‹¤íŒ¨: {e}")
            return None
    else:
        logging.error(f"ì¸ë±ìŠ¤ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {index_path}")
        return None

# FAISS ì¸ë±ìŠ¤ ë¡œë“œ
try:
    faiss_index = load_faiss_index(os.path.join(module_path, 'faiss_index_1.index'))
    if faiss_index is not None:
        logging.info("FAISS ì¸ë±ìŠ¤ ë¡œë“œ ì™„ë£Œ.")
    else:
        st.error("FAISS ì¸ë±ìŠ¤ ë¡œë“œì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤.")
        st.stop()
except FileNotFoundError as e:
    st.error(str(e))
    st.stop()
except Exception as e:
    st.error(f"FAISS ì¸ë±ìŠ¤ ë¡œë“œ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {e}")
    st.stop()

# í…ìŠ¤íŠ¸ ì„ë² ë”© ìƒì„± í•¨ìˆ˜ ì •ì˜
def embed_text(text):
    try:
        inputs = tokenizer(text, return_tensors='pt', padding=True, truncation=True).to(device)
        with torch.no_grad():
            embeddings = embedding_model(**inputs).last_hidden_state.mean(dim=1)
        return embeddings.squeeze().cpu().numpy()
    except Exception as e:
        logging.error(f"ì„ë² ë”© ìƒì„± ì‹¤íŒ¨: {e}")
        return None

# ì§ˆë¬¸ íŒŒì‹± í•¨ìˆ˜ ì •ì˜
def parse_question(question):
    """
    ì§ˆë¬¸ì„ íŒŒì‹±í•˜ì—¬ í•„í„°ë§ ê¸°ì¤€ì„ ì¶”ì¶œí•©ë‹ˆë‹¤.
    í˜„ì¬ëŠ” ìœ„ì¹˜, ì—°ë ¹ëŒ€, ìŒì‹ ì¢…ë¥˜, ê°€ê²©ëŒ€ë¥¼ ì¶”ì¶œí•©ë‹ˆë‹¤.
    í•„ìš”ì— ë”°ë¼ ì¶”ê°€ì ì¸ í•„í„°ë§ ê¸°ì¤€ì„ ì¶”ì¶œí•˜ë„ë¡ í™•ì¥í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
    """
    # ìœ„ì¹˜ íŒ¨í„´: ë‹¤ì–‘í•œ ìœ„ì¹˜ íŒ¨í„´ì„ ì¸ì‹
    location_pattern = r'([ê°€-í£]+ì‹œ [ê°€-í£]+ì|[ê°€-í£]+êµ¬ [ê°€-í£]+ë™)'
    location_match = re.search(location_pattern, question)
    location = location_match.group() if location_match else None
    
    # ì—°ë ¹ëŒ€ ì¶”ì¶œ: ëª¨ë“  ì—°ë ¹ëŒ€ ì¶”ì¶œ
    age_groups = re.findall(r'(\d+)ëŒ€', question)
    age_group = age_groups[0] if age_groups else None  # ì²« ë²ˆì§¸ ì—°ë ¹ëŒ€ë§Œ ì‚¬ìš©
    
    # ìŒì‹ ì¢…ë¥˜ ì¶”ì¶œ: ì˜ˆì‹œë¡œ 'ì»¤í”¼', 'ë””ì €íŠ¸', 'ìŠ¤í…Œì´í¬' ë“±
    food_type_match = re.search(r'(ì»¤í”¼|ë””ì €íŠ¸|ìŠ¤í…Œì´í¬|í•œì‹|ì¤‘ì‹|ì–‘ì‹|ì¼ì‹)', question)
    food_type = food_type_match.group() if food_type_match else None
    
    # ê°€ê²©ëŒ€ ì¶”ì¶œ: ì´ë¯¸ ì‚¬ì´ë“œë°”ì—ì„œ ì„ íƒëœ ê°€ê²©ëŒ€ ì‚¬ìš©
    # ì—¬ê¸°ì„œëŠ” ê¸°ë³¸ì ìœ¼ë¡œ 'ìƒê´€ ì—†ìŒ'ìœ¼ë¡œ ì„¤ì •
    price = 'ìƒê´€ ì—†ìŒ'
    
    return location, age_group, food_type, price

# Streamlit App UI ì„¤ì •
st.set_page_config(page_title="ğŸŠì œì£¼ë„ ë§›ì§‘ ì¶”ì²œ")

# ì‚¬ì´ë“œë°” ì„¤ì •
with st.sidebar:
    st.title("**ğŸŠì œì£¼ë„ ë§›ì§‘ ì¶”ì²œ**")
    st.write("")
    st.markdown("""
        <style>
        .sidebar-text {
        color: #FFEC9D;
        font-size: 18px;
        font-weight: bold;
        }
     </style>
     """, unsafe_allow_html=True)
    st.sidebar.markdown('<p class="sidebar-text">ğŸ’µí¬ë§ ê°€ê²©ëŒ€ëŠ” ì–´ë–»ê²Œ ë˜ì‹œë‚˜ìš”??</p>', unsafe_allow_html=True)

    price_options = ['ğŸ‘Œ ìƒê´€ ì—†ìŒ','ğŸ˜ ìµœê³ ê°€', 'ğŸ’¸ ê³ ê°€', 'ğŸ’° í‰ê·  ê°€ê²©ëŒ€', 'ğŸ’µ ì¤‘ì €ê°€', 'ğŸ˜‚ ì €ê°€']
    price_mapping = {
        'ğŸ‘Œ ìƒê´€ ì—†ìŒ': 'ìƒê´€ ì—†ìŒ',
        'ğŸ˜ ìµœê³ ê°€': 'ìµœê³ ê°€',
        'ğŸ’¸ ê³ ê°€': 'ê³ ê°€',
        'ğŸ’° í‰ê·  ê°€ê²©ëŒ€': 'í‰ê·  ê°€ê²©ëŒ€',
        'ğŸ’µ ì¤‘ì €ê°€': 'ì¤‘ì €ê°€',
        'ğŸ˜‚ ì €ê°€': 'ì €ê°€'
    }
    selected_price = st.sidebar.selectbox("", price_options, key="price")
    price = price_mapping.get(selected_price, 'ìƒê´€ ì—†ìŒ')

    st.markdown(
        """
         <style>
         [data-testid="stSidebar"] {
         background-color: #ff9900;
         }
         </style>
        """, unsafe_allow_html=True)
    st.write("")

st.title("ì–´ì„œ ì™€ìš©!ğŸ‘‹")
st.subheader("ì¸ê¸° ìˆëŠ” :orange[ì œì£¼ ë§›ì§‘]ğŸ½ï¸ğŸ˜ í›„íšŒëŠ” ì—†ì„ê±¸?!")

st.write("")
st.write("#í‘ë¼ì§€ #ì œì²  ìƒì„ íšŒ #í•´ë¬¼ë¼ë©´ #ìŠ¤í…Œì´í¬ #í•œì‹ #ì¤‘ì‹ #ì–‘ì‹ #ì¼ì‹ #í‘ë°±ìš”ë¦¬ì‚¬..ğŸ¤¤")

st.write("")

# ì´ë¯¸ì§€ ì¶”ê°€
image_path = "https://pimg.mk.co.kr/news/cms/202409/22/news-p.v1.20240922.a626061476c54127bbe4beb0aa12d050_P1.png"
image_html = f"""
<div style="display: flex; justify-content: center;">
    <img src="{image_path}" alt="centered image" width="70%">
</div>
"""
st.markdown(image_html, unsafe_allow_html=True)

st.write("")

# ëŒ€í™” ìƒíƒœ ì´ˆê¸°í™”
if "messages" not in st.session_state:
    st.session_state.messages = [{"role": "assistant", "content": "ì–´ë–¤ ì‹ë‹¹ ì°¾ìœ¼ì‹œë‚˜ìš”?? ìœ„ì¹˜, ì—…ì¢… ë“±ì„ ì•Œë ¤ì£¼ì‹œë©´ ìµœê³ ì˜ ë§›ì§‘ ì¶”ì²œí•´ë“œë¦´ê²Œìš”!"}]

# ë©”ì‹œì§€ ì¶œë ¥
for message in st.session_state.messages:
    with st.chat_message(message["role"]):
        st.write(message["content"])

# ì±— ê¸°ë¡ ì´ˆê¸°í™” ë²„íŠ¼
def clear_chat_history():
    st.session_state.messages = [{"role": "assistant", "content": "ì–´ë–¤ ì‹ë‹¹ ì°¾ìœ¼ì‹œë‚˜ìš”?? ìœ„ì¹˜, ì—…ì¢… ë“±ì„ ì•Œë ¤ì£¼ì‹œë©´ ìµœê³ ì˜ ë§›ì§‘ ì¶”ì²œí•´ë“œë¦´ê²Œìš”!"}]
st.sidebar.button('ëŒ€í™” ì´ˆê¸°í™” ğŸ”„', on_click=clear_chat_history)

# FAISSë¥¼ í™œìš©í•œ ì‘ë‹µ ìƒì„± í•¨ìˆ˜ ì •ì˜
def generate_response_with_faiss(question, df, faiss_index, model, df_tour, k=10, print_prompt=True):
    location, age_group, food_type, price = parse_question(question)
    
    # í•„í„°ë§ ê¸°ì¤€ì´ ì—†ì„ ê²½ìš° ê¸°ë³¸ê°’ ì„¤ì • ë˜ëŠ” ì‚¬ìš©ìì—ê²Œ ì¶”ê°€ ì •ë³´ ìš”ì²­
    if not location and not food_type and not age_group:
        return "ê²€ìƒ‰ì— í•„ìš”í•œ ì •ë³´ë¥¼ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤. ìœ„ì¹˜, ì—°ë ¹ëŒ€, ìŒì‹ ì¢…ë¥˜ ë“±ì„ í¬í•¨í•˜ì—¬ ë‹¤ì‹œ ì…ë ¥í•´ì£¼ì„¸ìš”."
    
    # ì„ë² ë”© ìƒì„±
    query_embedding = embed_text(question)
    if query_embedding is None:
        return "ì§ˆë¬¸ì— ëŒ€í•œ ì„ë² ë”©ì„ ìƒì„±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
    
    query_embedding = query_embedding.reshape(1, -1).astype('float32')
    logging.info(f"Query Embedding Shape: {query_embedding.shape}, Type: {query_embedding.dtype}")
    
    # FAISS ê²€ìƒ‰
    try:
        distances, indices = faiss_index.search(query_embedding, k)
        logging.info(f"FAISS ê²€ìƒ‰ ì™„ë£Œ: {k}ê°œ ê²°ê³¼")
        logging.info(f"Distances: {distances}")
        logging.info(f"Indices: {indices}")
    except Exception as e:
        logging.error(f"FAISS ê²€ìƒ‰ ì‹¤íŒ¨: {e}")
        return "FAISS ê²€ìƒ‰ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤."
    
    # ê²€ìƒ‰ ê²°ê³¼ê°€ ì—†ëŠ” ê²½ìš°
    if indices.size == 0 or len(indices[0]) == 0:
        return "ì§ˆë¬¸ê³¼ ì¼ì¹˜í•˜ëŠ” ê°€ê²Œê°€ ì—†ìŠµë‹ˆë‹¤."
    
    # ê²€ìƒ‰ëœ ê°€ê²Œë“¤ ì„ íƒ
    try:
        top_cafes = df.iloc[indices[0]].copy()
        logging.info(f"ê²€ìƒ‰ëœ ì¹´í˜ë“¤: {top_cafes['ê°€ë§¹ì ëª…'].tolist()}")
    except IndexError as e:
        logging.error(f"ì¸ë±ìŠ¤ ì´ˆê³¼ ì˜¤ë¥˜: {e}")
        return "ê²€ìƒ‰ëœ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤."
    
    # ì¶”ê°€ í•„í„°ë§
    if location:
        top_cafes = top_cafes[top_cafes['ê°€ë§¹ì ì£¼ì†Œ'].str.contains(location)]
    
    if food_type:
        top_cafes = top_cafes[top_cafes['ì—…ì¢…'].str.contains(food_type)]
    
    if price and price != 'ìƒê´€ ì—†ìŒ':
        top_cafes = top_cafes[top_cafes['ê±´ë‹¹í‰ê· ì´ìš©ê¸ˆì•¡êµ¬ê°„'].str.startswith(price_mapping.get(price, 'ìƒê´€ ì—†ìŒ'))]
    
    if age_group:
        # ì˜ˆì‹œ: 30ëŒ€ ì´ìƒ ë¹„ì¤‘ì´ ë†’ì€ ê°€ê²Œë¡œ í•„í„°ë§
        # ì‹¤ì œ ê¸°ì¤€ì— ë§ê²Œ ìˆ˜ì • í•„ìš”
        top_cafes = top_cafes[top_cafes['ìµœê·¼12ê°œì›”30ëŒ€íšŒì›ìˆ˜ë¹„ì¤‘'] >= 0.3]
    
    logging.info(f"ì¶”ê°€ í•„í„°ë§ í›„ ì¹´í˜ ìˆ˜: {len(top_cafes)}")
    
    if top_cafes.empty:
        return "ì§ˆë¬¸ê³¼ ì¼ì¹˜í•˜ëŠ” ê°€ê²Œê°€ ì—†ìŠµë‹ˆë‹¤."
    
    # ê°€ì¥ ë†’ì€ 30ëŒ€ ì´ìš© ë¹„ì¤‘ì„ ê°€ì§„ ì¹´í˜ ì„ íƒ
    try:
        top_cafe = top_cafes.loc[top_cafes['ìµœê·¼12ê°œì›”30ëŒ€íšŒì›ìˆ˜ë¹„ì¤‘'].idxmax()]
        reference_info = f"{top_cafe['ê°€ë§¹ì ëª…']} - {top_cafe['ê°€ë§¹ì ì£¼ì†Œ']} - 30ëŒ€ ë¹„ì¤‘: {top_cafe['ìµœê·¼12ê°œì›”30ëŒ€íšŒì›ìˆ˜ë¹„ì¤‘'] * 100:.1f}%"
        logging.info(f"ê°€ì¥ ë†’ì€ 30ëŒ€ ì´ìš© ë¹„ì¤‘ ì¹´í˜ ì„ íƒ: {top_cafe['ê°€ë§¹ì ëª…']}")
    except Exception as e:
        logging.error(f"30ëŒ€ ë¹„ì¤‘ ê¸°ì¤€ ì¹´í˜ ì„ íƒ ì‹¤íŒ¨: {e}")
        return "ê°€ì¥ ì í•©í•œ ê°€ê²Œë¥¼ ì°¾ëŠ” ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤."
    
    # ê´€ê´‘ì§€ ì •ë³´ í•„í„°ë§ (í•„ìš” ì‹œ ìˆ˜ì • ê°€ëŠ¥)
    reference_tour = "\n".join(df_tour['text'].iloc[:1])  # ì˜ˆì‹œ: ì²« ë²ˆì§¸ ê´€ê´‘ì§€ ì •ë³´
    
    prompt = f"""ì§ˆë¬¸: {question}
ëŒ€ë‹µì‹œ í•„ìš”í•œ ë‚´ìš©: 
- ê·¼ì²˜ ìŒì‹ì ì„ ì¶”ì²œí• ë•ŒëŠ” ì§ˆë¬¸ì— ì£¼ì†Œì— ëŒ€í•œ ì •ë³´ê°€ ìˆë‹¤ë©´ ìŒì‹ì ì˜ ì£¼ì†Œê°€ ë¹„ìŠ·í•œì§€ í™•ì¸í•´.
- ì°¨ë¡œ ì´ë™ì‹œê°„ì´ ì–¼ë§ˆì¸ì§€ ì•Œë ¤ì¤˜. ì¶”ì²œí•´ì¤„ë•Œ ì´ë™ì‹œê°„ì„ ê³ ë ¤í•´ì„œ ë‹µë³€í•´ì¤˜.
- ê°€ë§¹ì ì—…ì¢…ì´ ì»¤í”¼ì¸ ê°€ê²ŒëŠ” ì—…ì¢…ì´ ì¹´í˜ì•¼.
- ëŒ€ë‹µí•´ì¤„ë•Œ ì—…ì¢…ë³„ë¡œ ê°€ëŠ¥í•˜ë©´ í•˜ë‚˜ì”© ì¶”ì²œí•´ì¤˜.
- ê·¸ë¦¬ê³  ì¶”ê°€ì ìœ¼ë¡œ ê·¸ ì¤‘ì—ì„œ ê°€ë§¹ì ê°œì ì¼ìê°€ ì˜¤ë˜ë˜ê³  ì´ìš©ê±´ìˆ˜ê°€ ë§ì€ ìŒì‹ì (ì˜¤ë˜ëœë§›ì§‘)ê³¼ ê°€ë§¹ì ê°œì ì¼ìê°€ ìµœê·¼ì´ê³  ì´ìš©ê±´ìˆ˜ê°€ ë§ì€ ìŒì‹ì (ìƒˆë¡œìš´ë§›ì§‘)ì„ ê°ê° ì¶”ì²œí•´ì¤¬ìœ¼ë©´ ì¢‹ê² ì–´.
ì°¸ê³ í•  ì •ë³´: {reference_info}
ì°¸ê³ í•  ê´€ê´‘ì§€ ì •ë³´: {reference_tour}
ì‘ë‹µ:"""
    
    if print_prompt:
        st.write('-----------------------------' * 3)
        st.write(prompt)
        st.write('-----------------------------' * 3)
    
    # ëª¨ë¸ ì‘ë‹µ ìƒì„±
    try:
        response = model.generate_content(prompt)
        logging.info("ëª¨ë¸ ì‘ë‹µ ìƒì„± ì™„ë£Œ.")
        return response.text if hasattr(response, 'text') else response
    except Exception as e:
        logging.error(f"ëª¨ë¸ ì‘ë‹µ ìƒì„± ì‹¤íŒ¨: {e}")
        return "ëª¨ë¸ ì‘ë‹µ ìƒì„± ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤."

# ì‚¬ìš©ì ì…ë ¥ ì²˜ë¦¬ ë° ì‘ë‹µ ìƒì„±
if prompt := st.chat_input():
    st.session_state.messages.append({"role": "user", "content": prompt})
    with st.chat_message("user"):
        st.write(prompt)

    if st.session_state.messages[-1]["role"] != "assistant":
        with st.chat_message("assistant"):
            with st.spinner("ìƒê° ì¤‘..."):
                response = generate_response_with_faiss(prompt, df, faiss_index, model, df_tour, k=10)
                st.write(response)
        st.session_state.messages.append({"role": "assistant", "content": response})
        
        # ë¡œê·¸ ê¸°ë¡
        logging.info(f"Question: {prompt}")
        logging.info(f"Answer: {response}")   
