import os
import numpy as np
import pandas as pd
from transformers import AutoTokenizer, AutoModel
import torch
import faiss
import streamlit as st
import google.generativeai as genai
import logging

# ê²½ë¡œ ì„¤ì •
data_path = './data'
module_path = './modules'

# ë¡œê·¸ ì„¤ì •
logging.basicConfig(filename='chatbot_logs.log', level=logging.INFO, 
                    format='%(asctime)s:%(levelname)s:%(message)s')

# Gemini ëª¨ë¸ ì„¤ì •
GOOGLE_API_KEY = st.secrets["API_KEY"]
genai.configure(api_key=GOOGLE_API_KEY)
model = genai.GenerativeModel("gemini-1.5-flash")

# ë°ì´í„° ë¡œë“œ
df = pd.read_csv(os.path.join(data_path, "JEJU_DATA.csv"), encoding='cp949')
df_tour = pd.read_csv(os.path.join(data_path, "JEJU_TOUR.csv"), encoding='cp949')
text_tour = df_tour['text'].tolist()

# ìµœì‹ ì—°ì›” ë°ì´í„°ë§Œ ì‚¬ìš©
df = df.loc[df.groupby('ê°€ë§¹ì ëª…')['ê¸°ì¤€ì—°ì›”'].idxmax()].reset_index(drop=True)

# 'text' ì»¬ëŸ¼ ì¡´ì¬ í™•ì¸
if 'text' not in df.columns:
    st.error("ë°ì´í„°ì…‹ì— 'text' ì»¬ëŸ¼ì´ ì—†ìŠµë‹ˆë‹¤.")
    st.stop()

# ë””ë°”ì´ìŠ¤ ì„¤ì •
device = "cuda" if torch.cuda.is_available() else "cpu"

# Hugging Face ì„ë² ë”© ëª¨ë¸ ë° í† í¬ë‚˜ì´ì € ë¡œë“œ
model_name = "jhgan/ko-sroberta-multitask"
tokenizer = AutoTokenizer.from_pretrained(model_name)
embedding_model = AutoModel.from_pretrained(model_name).to(device)

# FAISS ì¸ë±ìŠ¤ ë¡œë“œ í•¨ìˆ˜
def load_faiss_index(index_path=os.path.join(module_path, 'faiss_index_1.index')):
    if os.path.exists(index_path):
        index = faiss.read_index(index_path)
        logging.info(f"FAISS ì¸ë±ìŠ¤ ë¡œë“œ ì„±ê³µ: {index_path}")
        return index
    else:
        logging.error(f"ì¸ë±ìŠ¤ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {index_path}")
        return None

# í…ìŠ¤íŠ¸ ì„ë² ë”© ìƒì„±
def embed_text(text):
    inputs = tokenizer(text, return_tensors='pt', padding=True, truncation=True).to(device)
    with torch.no_grad():
        embeddings = embedding_model(**inputs).last_hidden_state.mean(dim=1)
    return embeddings.squeeze().cpu().numpy()

# Streamlit App UI ì„¤ì •
st.set_page_config(page_title="ğŸŠì œì£¼ë„ ë§›ì§‘ ì¶”ì²œ")

# ì‚¬ì´ë“œë°” ì„¤ì •
with st.sidebar:
    st.title("**ğŸŠì œì£¼ë„ ë§›ì§‘ ì¶”ì²œ**")
    st.sidebar.markdown('<p class="sidebar-text">ğŸ’µí¬ë§ ê°€ê²©ëŒ€ëŠ” ì–´ë–»ê²Œ ë˜ì‹œë‚˜ìš”??</p>', unsafe_allow_html=True)

    price_options = ['ğŸ‘Œ ìƒê´€ ì—†ìŒ', 'ğŸ˜ ìµœê³ ê°€', 'ğŸ’¸ ê³ ê°€', 'ğŸ’° í‰ê·  ê°€ê²©ëŒ€', 'ğŸ’µ ì¤‘ì €ê°€', 'ğŸ˜‚ ì €ê°€']
    price_mapping = {
        'ğŸ‘Œ ìƒê´€ ì—†ìŒ': 'ìƒê´€ ì—†ìŒ',
        'ğŸ˜ ìµœê³ ê°€': '6',
        'ğŸ’¸ ê³ ê°€': '5',
        'ğŸ’° í‰ê·  ê°€ê²©ëŒ€': ('3', '4'),
        'ğŸ’µ ì¤‘ì €ê°€': '2',
        'ğŸ˜‚ ì €ê°€': '1'
    }
    selected_price = st.sidebar.selectbox("", price_options, key="price")
    price = price_mapping.get(selected_price, 'ìƒê´€ ì—†ìŒ')

st.title("ì–´ì„œ ì™€ìš©!ğŸ‘‹")
st.subheader("ì¸ê¸° ìˆëŠ” :orange[ì œì£¼ ë§›ì§‘]ğŸ½ï¸ğŸ˜ í›„íšŒëŠ” ì—†ì„ê±¸?!")

# ëŒ€í™” ìƒíƒœ ì´ˆê¸°í™”
if "messages" not in st.session_state:
    st.session_state.messages = [{"role": "assistant", "content": "ì–´ë–¤ ì‹ë‹¹ ì°¾ìœ¼ì‹œë‚˜ìš”?? ìœ„ì¹˜, ì—…ì¢… ë“±ì„ ì•Œë ¤ì£¼ì‹œë©´ ìµœê³ ì˜ ë§›ì§‘ ì¶”ì²œí•´ë“œë¦´ê²Œìš”!"}]

# ë©”ì‹œì§€ ì¶œë ¥
for message in st.session_state.messages:
    with st.chat_message(message["role"]):
        st.write(message["content"])

# ì±— ê¸°ë¡ ì´ˆê¸°í™” ë²„íŠ¼
def clear_chat_history():
    st.session_state.messages = [{"role": "assistant", "content": "ì–´ë–¤ ì‹ë‹¹ ì°¾ìœ¼ì‹œë‚˜ìš”?? ìœ„ì¹˜, ì—…ì¢… ë“±ì„ ì•Œë ¤ì£¼ì‹œë©´ ìµœê³ ì˜ ë§›ì§‘ ì¶”ì²œí•´ë“œë¦´ê²Œìš”!"}]
st.sidebar.button('ëŒ€í™” ì´ˆê¸°í™” ğŸ”„', on_click=clear_chat_history)

# FAISSë¥¼ í™œìš©í•œ ì‘ë‹µ ìƒì„± í•¨ìˆ˜ ì •ì˜
def generate_response_with_faiss(question, df, model, df_tour, k=3):
    index = load_faiss_index()
    
    if index is None:
        return "FAISS ì¸ë±ìŠ¤ë¥¼ ë¡œë“œí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
    
    query_embedding = embed_text(question).reshape(1, -1)
    distances, indices = index.search(query_embedding, k)

    filtered_df = df.iloc[indices[0, :]].reset_index(drop=True)

    # ê°€ê²©ëŒ€ í•„í„°ë§
    if price != 'ìƒê´€ ì—†ìŒ':
        if isinstance(price, tuple):
            filtered_df = filtered_df[filtered_df['ê±´ë‹¹í‰ê· ì´ìš©ê¸ˆì•¡êµ¬ê°„'].str.startswith(price)].reset_index(drop=True)
        else:
            filtered_df = filtered_df[filtered_df['ê±´ë‹¹í‰ê· ì´ìš©ê¸ˆì•¡êµ¬ê°„'].str.startswith(price)].reset_index(drop=True)

    if filtered_df.empty:
        return "ì§ˆë¬¸ê³¼ ì¼ì¹˜í•˜ëŠ” ê°€ê²Œê°€ ì—†ìŠµë‹ˆë‹¤."

    reference_info = "\n".join(filtered_df['text'])
    reference_tour = "\n".join(df_tour['text'].iloc[:1])  # ì²« ë²ˆì§¸ ê´€ê´‘ì§€ ì •ë³´

    prompt = f"""ì§ˆë¬¸: {question}\nëŒ€ë‹µì‹œ í•„ìš”í•œ ë‚´ìš©: ê·¼ì²˜ ìŒì‹ì ì„ ì¶”ì²œí• ë•ŒëŠ” ì§ˆë¬¸ì— ì£¼ì†Œì— ëŒ€í•œ ì •ë³´ê°€ ìˆë‹¤ë©´ ìŒì‹ì ì˜ ì£¼ì†Œê°€ ë¹„ìŠ·í•œì§€ í™•ì¸í•´.\nì°¨ë¡œ ì´ë™ì‹œê°„ì´ ì–¼ë§ˆì¸ì§€ ì•Œë ¤ì¤˜. ì¶”ì²œí•´ì¤„ë•Œ ì´ë™ì‹œê°„ì„ ê³ ë ¤í•´ì„œ ë‹µë³€í•´ì¤˜.\nê°€ë§¹ì ì—…ì¢…ì´ ì»¤í”¼ì¸ ê°€ê²ŒëŠ” ì—…ì¢…ì´ ì¹´í˜ì•¼.\nëŒ€ë‹µí•´ì¤„ë•Œ ì—…ì¢…ë³„ë¡œ ê°€ëŠ¥í•˜ë©´ í•˜ë‚˜ì”© ì¶”ì²œí•´ì¤˜. ê·¸ë¦¬ê³  ì¶”ê°€ì ìœ¼ë¡œ ê·¸ ì¤‘ì—ì„œ ê°€ë§¹ì ê°œì ì¼ìê°€ ì˜¤ë˜ë˜ê³  ì´ìš©ê±´ìˆ˜ê°€ ë§ì€ ìŒì‹ì (ì˜¤ë˜ëœë§›ì§‘)ê³¼ ê°€ë§¹ì ê°œì ì¼ìê°€ ìµœê·¼ì´ê³  ì´ìš©ê±´ìˆ˜ê°€ ë§ì€ ìŒì‹ì (ìƒˆë¡œìš´ë§›ì§‘)ì„ ê°ê° ì¶”ì²œí•´ì¤¬ìœ¼ë©´ ì¢‹ê² ì–´.\nì°¸ê³ í•  ì •ë³´: {reference_info}\nì°¸ê³ í•  ê´€ê´‘ì§€ ì •ë³´: {reference_tour}\nì‘ë‹µ:"""

    response = model.generate_content(prompt)
    return response.text if hasattr(response, 'text') else response

# ì‚¬ìš©ì ì…ë ¥ ì²˜ë¦¬ ë° ì‘ë‹µ ìƒì„±
if prompt := st.chat_input():
    st.session_state.messages.append({"role": "user", "content": prompt})
    with st.chat_message("user"):
        st.write(prompt)

    if st.session_state.messages[-1]["role"] != "assistant":
        with st.chat_message("assistant"):
            with st.spinner("ìƒê° ì¤‘..."):
                response = generate_response_with_faiss(prompt, df, model, df_tour)
                st.write(response)
        st.session_state.messages.append({"role": "assistant", "content": response})
